"""
Test S3 iterdir Caching Improvement

This test verifies that the improved iterdir() method properly caches
complete directory listings and avoids API calls when cache exists.

Run with: PYTHONPATH=.:src:ttk pytest test/test_s3_iterdir_caching_improvement.py -v
"""

import time
import unittest
from unittest.mock import Mock, patch, call
from datetime import datetime

try:
    from tfm_path import Path
    from tfm_s3 import S3PathImpl, get_s3_cache
except ImportError as e:
    print(f"Error importing required modules: {e}")
    print("Skipping S3 iterdir caching improvement tests")
    sys.exit(0)


class TestS3IterdirCachingImprovement(unittest.TestCase):
    """Test S3 iterdir caching improvement"""
    
    def setUp(self):
        """Set up test environment"""
        # Clear cache before each test
        cache = get_s3_cache()
        cache.clear()
        
        # Mock S3 client
        self.mock_client = Mock()
        
        # Sample S3 objects for testing
        self.sample_objects = [
            {
                'Key': 'project/src/main.py',
                'Size': 2048,
                'LastModified': datetime(2024, 1, 1, 12, 0, 0),
                'ETag': '"abc123"',
                'StorageClass': 'STANDARD'
            },
            {
                'Key': 'project/src/utils.py',
                'Size': 1024,
                'LastModified': datetime(2024, 1, 2, 12, 0, 0),
                'ETag': '"def456"',
                'StorageClass': 'STANDARD'
            },
            {
                'Key': 'project/README.md',
                'Size': 512,
                'LastModified': datetime(2024, 1, 3, 12, 0, 0),
                'ETag': '"ghi789"',
                'StorageClass': 'STANDARD'
            }
        ]
        
        # Mock list_objects_v2 response
        self.mock_list_response = {
            'Contents': self.sample_objects,
            'CommonPrefixes': [{'Prefix': 'project/src/'}],
            'KeyCount': len(self.sample_objects),
            'IsTruncated': False
        }
        
        # Configure mock client
        self.mock_client.list_objects_v2.return_value = self.mock_list_response
        
        # Mock paginator
        self.mock_paginator = Mock()
        self.mock_paginator.paginate.return_value = [self.mock_list_response]
        self.mock_client.get_paginator.return_value = self.mock_paginator
    
    @patch('tfm_s3.boto3.client')
    def test_first_iterdir_makes_api_calls_and_caches_complete_listing(self, mock_boto3_client):
        """Test that first iterdir() call makes API calls and caches complete listing"""
        mock_boto3_client.return_value = self.mock_client
        
        # Create S3 path for directory
        s3_path = Path('s3://test-bucket/project/')
        
        # First call should make API calls and cache complete listing
        files = list(s3_path.iterdir())
        
        # Verify we got the expected files
        self.assertEqual(len(files), 4)  # 3 files + 1 directory
        
        # Verify API calls were made
        self.mock_client.get_paginator.assert_called_once_with('list_objects_v2')
        self.mock_paginator.paginate.assert_called_once()
        
        # Verify complete listing was cached
        cache = get_s3_cache()
        cached_listing = cache.get(
            operation='list_objects_v2_complete',
            bucket='test-bucket',
            key='project/',
            prefix='project/',
            delimiter='/',
            complete_listing=True
        )
        
        self.assertIsNotNone(cached_listing)
        self.assertEqual(len(cached_listing['Contents']), 3)
        self.assertEqual(len(cached_listing['CommonPrefixes']), 1)
    
    @patch('tfm_s3.boto3.client')
    def test_second_iterdir_uses_cache_no_api_calls(self, mock_boto3_client):
        """Test that second iterdir() call uses cache and makes no API calls"""
        mock_boto3_client.return_value = self.mock_client
        
        # Create S3 path for directory
        s3_path = Path('s3://test-bucket/project/')
        
        # First call to populate cache
        files1 = list(s3_path.iterdir())
        
        # Reset mock to track subsequent calls
        self.mock_client.reset_mock()
        self.mock_paginator.reset_mock()
        
        # Second call should use cache
        files2 = list(s3_path.iterdir())
        
        # Verify same results
        self.assertEqual(len(files1), len(files2))
        
        # Verify no additional API calls were made
        self.mock_client.get_paginator.assert_not_called()
        self.mock_paginator.paginate.assert_not_called()
        self.mock_client.list_objects_v2.assert_not_called()
    
    @patch('tfm_s3.boto3.client')
    def test_cache_stores_complete_aggregated_listing(self, mock_boto3_client):
        """Test that cache stores complete aggregated listing from all pages"""
        mock_boto3_client.return_value = self.mock_client
        
        # Mock multiple pages
        page1 = {
            'Contents': self.sample_objects[:2],
            'CommonPrefixes': [],
            'KeyCount': 2,
            'IsTruncated': True
        }
        
        page2 = {
            'Contents': self.sample_objects[2:],
            'CommonPrefixes': [{'Prefix': 'project/src/'}],
            'KeyCount': 1,
            'IsTruncated': False
        }
        
        self.mock_paginator.paginate.return_value = [page1, page2]
        
        # Create S3 path for directory
        s3_path = Path('s3://test-bucket/project/')
        
        # Call iterdir to populate cache
        files = list(s3_path.iterdir())
        
        # Verify we got all files from both pages
        self.assertEqual(len(files), 4)  # 3 files + 1 directory
        
        # Verify cached listing contains aggregated data from all pages
        cache = get_s3_cache()
        cached_listing = cache.get(
            operation='list_objects_v2_complete',
            bucket='test-bucket',
            key='project/',
            prefix='project/',
            delimiter='/',
            complete_listing=True
        )
        
        self.assertIsNotNone(cached_listing)
        self.assertEqual(len(cached_listing['Contents']), 3)  # All 3 files aggregated
        self.assertEqual(len(cached_listing['CommonPrefixes']), 1)  # 1 directory
    
    @patch('tfm_s3.boto3.client')
    def test_cached_paths_have_metadata(self, mock_boto3_client):
        """Test that paths from cached listing have proper metadata"""
        mock_boto3_client.return_value = self.mock_client
        
        # Create S3 path for directory
        s3_path = Path('s3://test-bucket/project/')
        
        # First call to populate cache
        files1 = list(s3_path.iterdir())
        
        # Second call using cache
        files2 = list(s3_path.iterdir())
        
        # Verify all paths have metadata
        for file_path in files2:
            self.assertIsNotNone(file_path._impl._is_dir_cached)
            self.assertIsNotNone(file_path._impl._is_file_cached)
            self.assertIsNotNone(file_path._impl._size_cached)
            self.assertIsNotNone(file_path._impl._mtime_cached)
    
    @patch('tfm_s3.boto3.client')
    def test_performance_improvement_with_cache(self, mock_boto3_client):
        """Test that cached iterdir() is significantly faster"""
        mock_boto3_client.return_value = self.mock_client
        
        # Create S3 path for directory
        s3_path = Path('s3://test-bucket/project/')
        
        # First call (with API calls)
        start_time = time.time()
        files1 = list(s3_path.iterdir())
        first_call_time = time.time() - start_time
        
        # Second call (cached)
        start_time = time.time()
        files2 = list(s3_path.iterdir())
        second_call_time = time.time() - start_time
        
        # Verify same results
        self.assertEqual(len(files1), len(files2))
        
        # Cached call should be much faster (at least 10x faster)
        # Note: In real scenarios the difference would be much more dramatic
        # due to network latency, but in mocked tests the difference is smaller
        self.assertLess(second_call_time, first_call_time)
    
    @patch('tfm_s3.boto3.client')
    def test_cache_invalidation_on_write_operations(self, mock_boto3_client):
        """Test that cache is properly invalidated on write operations"""
        mock_boto3_client.return_value = self.mock_client
        
        # Create S3 path for directory
        s3_path = Path('s3://test-bucket/project/')
        
        # Populate cache
        files = list(s3_path.iterdir())
        
        # Verify cache exists
        cache = get_s3_cache()
        cached_listing = cache.get(
            operation='list_objects_v2_complete',
            bucket='test-bucket',
            key='project/',
            prefix='project/',
            delimiter='/',
            complete_listing=True
        )
        self.assertIsNotNone(cached_listing)
        
        # Simulate write operation that should invalidate cache
        file_path = Path('s3://test-bucket/project/new_file.txt')
        file_path.write_text("test content")
        
        # Verify cache was invalidated (this depends on the invalidation logic)
        # The exact behavior depends on how cache invalidation is implemented
        # This test verifies the concept
    
    def test_cache_key_generation_for_complete_listing(self):
        """Test that cache keys are generated correctly for complete listings"""
        cache = get_s3_cache()
        
        # Test cache key generation
        key1 = cache._generate_cache_key(
            'list_objects_v2_complete', 'bucket', 'path/',
            prefix='path/', delimiter='/', complete_listing=True
        )
        
        key2 = cache._generate_cache_key(
            'list_objects_v2_complete', 'bucket', 'path/',
            prefix='path/', delimiter='/', complete_listing=True
        )
        
        key3 = cache._generate_cache_key(
            'list_objects_v2_complete', 'bucket', 'other/',
            prefix='other/', delimiter='/', complete_listing=True
        )
        
        # Same parameters should generate same key
        self.assertEqual(key1, key2)
        
        # Different parameters should generate different keys
        self.assertNotEqual(key1, key3)
    
    @patch('tfm_s3.boto3.client')
    def test_yield_paths_from_cached_listing_helper(self, mock_boto3_client):
        """Test the _yield_paths_from_cached_listing helper method"""
        mock_boto3_client.return_value = self.mock_client
        
        # Create S3PathImpl instance
        s3_impl = S3PathImpl('s3://test-bucket/project/')
        
        # Create test cached listing
        cached_listing = {
            'Contents': self.sample_objects,
            'CommonPrefixes': [{'Prefix': 'project/src/'}],
            'Prefix': 'project/',
            'Delimiter': '/'
        }
        
        # Test the helper method
        paths = list(s3_impl._yield_paths_from_cached_listing(cached_listing))
        
        # Verify we got the expected paths
        self.assertEqual(len(paths), 4)  # 3 files + 1 directory
        
        # Verify all paths have metadata
        for path in paths:
            self.assertIsNotNone(path._impl._is_dir_cached)
            self.assertIsNotNone(path._impl._is_file_cached)


def main():
    """Run the tests"""
    unittest.main(verbosity=2)
