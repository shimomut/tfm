#!/usr/bin/env python3
"""
Test S3 Caching Optimization

This test verifies that the S3 caching optimization reduces API calls
during directory listing and file stat operations.
"""

import sys
import os
import time
import unittest
from unittest.mock import Mock, patch, MagicMock
from datetime import datetime

# Add src directory to path for imports
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))

try:
    from tfm_path import Path
    from tfm_s3 import S3PathImpl, get_s3_cache
except ImportError as e:
    print(f"Error importing required modules: {e}")
    print("Skipping S3 caching optimization tests")
    sys.exit(0)


class TestS3CachingOptimization(unittest.TestCase):
    """Test S3 caching optimization"""
    
    def setUp(self):
        """Set up test environment"""
        # Clear cache before each test
        cache = get_s3_cache()
        cache.clear()
        
        # Mock S3 client
        self.mock_client = Mock()
        self.mock_paginator = Mock()
        self.mock_client.get_paginator.return_value = self.mock_paginator
        
        # Sample S3 objects for testing
        self.sample_objects = [
            {
                'Key': 'folder/file1.txt',
                'Size': 1024,
                'LastModified': datetime(2024, 1, 1, 12, 0, 0),
                'ETag': '"abc123"',
                'StorageClass': 'STANDARD'
            },
            {
                'Key': 'folder/file2.txt', 
                'Size': 2048,
                'LastModified': datetime(2024, 1, 2, 12, 0, 0),
                'ETag': '"def456"',
                'StorageClass': 'STANDARD'
            },
            {
                'Key': 'folder/subdir/',
                'Size': 0,
                'LastModified': datetime(2024, 1, 3, 12, 0, 0),
                'ETag': '"ghi789"',
                'StorageClass': 'STANDARD'
            }
        ]
        
        # Mock paginator response
        self.mock_page = {
            'Contents': self.sample_objects,
            'CommonPrefixes': [{'Prefix': 'folder/subdir/'}],
            'KeyCount': len(self.sample_objects),
            'IsTruncated': False
        }
        
        self.mock_paginator.paginate.return_value = [self.mock_page]
    
    @patch('tfm_s3.boto3.client')
    def test_iterdir_caches_stat_info(self, mock_boto3_client):
        """Test that iterdir caches stat information from directory listing"""
        mock_boto3_client.return_value = self.mock_client
        
        # Create S3 path
        s3_path = Path('s3://test-bucket/folder/')
        
        # List directory contents
        files = list(s3_path.iterdir())
        
        # Verify we got the expected files
        self.assertEqual(len(files), 3)  # 2 files + 1 directory
        
        # Verify paginator was called once
        self.mock_client.get_paginator.assert_called_once_with('list_objects_v2')
        self.mock_paginator.paginate.assert_called_once()
        
        # Now test that stat() calls don't make additional API calls
        cache = get_s3_cache()
        
        # Check cache contains head_object entries for files
        file_paths = [f for f in files if f.is_file()]
        
        for file_path in file_paths:
            # This should use cached data, not make API calls
            stat_result = file_path.stat()
            
            # Verify stat result has correct data
            self.assertGreater(stat_result.st_size, 0)
            self.assertGreater(stat_result.st_mtime, 0)
        
        # Verify no additional API calls were made for head_object
        # (The mock client should only have been called for list_objects_v2)
        self.mock_client.head_object.assert_not_called()
    
    @patch('tfm_s3.boto3.client')
    def test_virtual_directory_stats_uses_cached_data(self, mock_boto3_client):
        """Test that virtual directory stats use cached directory listing data"""
        mock_boto3_client.return_value = self.mock_client
        
        # Create S3 path for directory
        s3_path = Path('s3://test-bucket/folder/')
        
        # First, list directory to populate cache
        files = list(s3_path.iterdir())
        
        # Reset mock to track subsequent calls
        self.mock_client.reset_mock()
        self.mock_paginator.reset_mock()
        
        # Now get directory stats - should use cached data
        stat_result = s3_path.stat()
        
        # Verify we got valid stats
        self.assertEqual(stat_result.st_size, 0)  # Directories have 0 size
        self.assertGreater(stat_result.st_mtime, 0)
        
        # Verify no additional API calls were made
        # (Should use cached directory listing data)
        self.mock_client.list_objects_v2.assert_not_called()
        self.mock_client.get_paginator.assert_not_called()
    
    @patch('tfm_s3.boto3.client')
    def test_cache_hit_ratio_improvement(self, mock_boto3_client):
        """Test that cache hit ratio improves with optimization"""
        mock_boto3_client.return_value = self.mock_client
        
        # Create S3 path
        s3_path = Path('s3://test-bucket/folder/')
        
        # Get cache instance
        cache = get_s3_cache()
        
        # First iteration - should populate cache
        files = list(s3_path.iterdir())
        
        # Get stats for all files (should use cached data)
        for file_path in files:
            if file_path.is_file():
                file_path.stat()
        
        # Second iteration - should be fully cached
        files2 = list(s3_path.iterdir())
        
        # Get stats again (should be fully cached)
        for file_path in files2:
            if file_path.is_file():
                file_path.stat()
        
        # Verify cache has entries
        cache_stats = cache.get_stats()
        self.assertGreater(cache_stats['total_entries'], 0)
        
        # Verify minimal API calls were made
        # Should only be called once for the initial directory listing
        self.assertEqual(self.mock_paginator.paginate.call_count, 1)
        self.mock_client.head_object.assert_not_called()
    
    def test_cache_key_generation(self):
        """Test that cache keys are generated correctly"""
        cache = get_s3_cache()
        
        # Test cache key generation for different operations
        key1 = cache._generate_cache_key('head_object', 'bucket', 'key1')
        key2 = cache._generate_cache_key('head_object', 'bucket', 'key2')
        key3 = cache._generate_cache_key('list_objects_v2', 'bucket', 'key1')
        
        # Keys should be different for different parameters
        self.assertNotEqual(key1, key2)
        self.assertNotEqual(key1, key3)
        
        # Same parameters should generate same key
        key1_duplicate = cache._generate_cache_key('head_object', 'bucket', 'key1')
        self.assertEqual(key1, key1_duplicate)
    
    def test_cache_invalidation(self):
        """Test that cache invalidation works correctly"""
        cache = get_s3_cache()
        
        # Add some test data to cache
        cache.put('head_object', 'bucket', 'key1', {'test': 'data1'})
        cache.put('head_object', 'bucket', 'key2', {'test': 'data2'})
        cache.put('list_objects_v2', 'bucket', '', {'test': 'list_data'})
        
        # Verify data is in cache
        self.assertIsNotNone(cache.get('head_object', 'bucket', 'key1'))
        self.assertIsNotNone(cache.get('head_object', 'bucket', 'key2'))
        self.assertIsNotNone(cache.get('list_objects_v2', 'bucket', ''))
        
        # Invalidate specific key
        cache.invalidate_key('bucket', 'key1')
        
        # Verify only key1 was invalidated
        self.assertIsNone(cache.get('head_object', 'bucket', 'key1'))
        self.assertIsNotNone(cache.get('head_object', 'bucket', 'key2'))
        self.assertIsNotNone(cache.get('list_objects_v2', 'bucket', ''))
        
        # Invalidate entire bucket
        cache.invalidate_bucket('bucket')
        
        # Verify all entries for bucket were invalidated
        self.assertIsNone(cache.get('head_object', 'bucket', 'key2'))
        self.assertIsNone(cache.get('list_objects_v2', 'bucket', ''))


def main():
    """Run the tests"""
    unittest.main(verbosity=2)


if __name__ == '__main__':
    main()