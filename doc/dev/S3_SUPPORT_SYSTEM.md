# TFM S3 Support System

## Overview

TFM provides comprehensive AWS S3 support through an extended Path implementation, allowing users to navigate, browse, and manipulate S3 buckets and objects using the same interface as local file operations. The system includes intelligent caching for performance optimization and modular architecture for maintainability.

## Features

### Core S3 Support
- **S3 URI Format**: `s3://bucket-name/key/path`
- **Seamless Integration**: S3 paths work alongside local paths
- **Pathlib Compatibility**: All standard pathlib operations supported
- **Cross-Storage Operations**: Copy/move between local and S3 storage

### Supported Operations

#### Navigation and Browsing
- Navigate to S3 buckets: `s3://my-bucket/`
- Browse S3 objects like local files
- List bucket contents and nested "directories"
- Support for S3 prefix-based directory simulation

#### File Operations
- **Read Operations**: `read_text()`, `read_bytes()`, `open()`
- **Write Operations**: `write_text()`, `write_bytes()`, `open('w')`
- **File Info**: `stat()`, `exists()`, `is_file()`, `is_dir()`
- **File Management**: `unlink()`, `rename()`, `touch()`

#### Path Manipulation
- **Path Building**: `joinpath()`, `/` operator
- **Name Changes**: `with_name()`, `with_suffix()`, `with_stem()`
- **Path Properties**: `name`, `stem`, `suffix`, `parent`, `parts`

#### Directory Operations
- **Listing**: `iterdir()`, `glob()`, `rglob()`
- **Creation**: `mkdir()` (creates directory markers)
- **Removal**: `rmdir()` (removes empty directories)

### Performance Caching System

#### Intelligent Caching
- **Automatic caching** of all S3 API calls (head_object, list_objects_v2, get_object, etc.)
- **Configurable TTL** with default of 60 seconds
- **Thread-safe operations** using RLock for concurrent access
- **LRU eviction** to manage memory usage with configurable max entries

#### Cache Invalidation
- **Automatic invalidation** on write operations (put_object, delete_object, etc.)
- **Partial invalidation** for specific keys or buckets
- **Parent directory invalidation** when files are modified
- **Prefix-based invalidation** for related cache entries

#### Performance Benefits
- **Reduced API calls** by up to 90% for repeated operations
- **Faster directory listings** with paginated cache support
- **Improved file stat operations** through cached head_object calls
- **Better user experience** with reduced latency

## Architecture

### Modular Design
The S3 support is implemented using a clean modular architecture:

```
src/
├── tfm_path.py          # Core path implementation (PathImpl, LocalPathImpl, Path)
├── tfm_s3.py           # S3 implementation (S3PathImpl, S3Cache, utilities)
└── _config.py          # Configuration (includes S3 tools)
```

### Core Components

#### S3PathImpl Class (`src/tfm_s3.py`)
- **Purpose**: Implements all PathImpl abstract methods for S3 operations
- **Key Features**:
  - Full pathlib-compatible interface
  - boto3 integration for AWS S3 operations
  - Lazy S3 client initialization
  - Integrated caching system
  - Proper error handling for AWS-specific errors

#### S3Cache Class
```python
class S3Cache:
    def __init__(self, default_ttl: int = 60, max_entries: int = 1000)
    def get(self, operation: str, bucket: str, key: str = "", **kwargs) -> Optional[Any]
    def put(self, operation: str, bucket: str, key: str = "", data: Any = None, ttl: Optional[int] = None, **kwargs)
    def invalidate_bucket(self, bucket: str)
    def invalidate_key(self, bucket: str, key: str)
    def invalidate_prefix(self, bucket: str, prefix: str)
    def clear(self)
    def get_stats(self) -> Dict[str, Any]
```

#### Path Factory Pattern (`src/tfm_path.py`)
```python
def _create_implementation(self, path_str: str) -> PathImpl:
    if path_str.startswith('s3://'):
        try:
            from .tfm_s3 import S3PathImpl  # Dynamic import
        except ImportError:
            from tfm_s3 import S3PathImpl   # Fallback for direct execution
        return S3PathImpl(path_str)
    return LocalPathImpl(PathlibPath(path_str))
```

### Cache Architecture
```
S3Cache
├── _cache: Dict[str, Dict[str, Any]]  # Main cache storage
├── _lock: threading.RLock             # Thread safety
├── default_ttl: int                   # Default cache TTL
└── max_entries: int                   # Maximum cache entries
```

## Installation and Configuration

### Dependencies
```bash
pip install boto3
```

### AWS Configuration
```bash
# Option 1: AWS CLI
aws configure

# Option 2: Environment Variables
export AWS_ACCESS_KEY_ID=your_access_key
export AWS_SECRET_ACCESS_KEY=your_secret_key
export AWS_DEFAULT_REGION=us-west-2

# Option 3: IAM Roles (for EC2 instances)
# Automatically detected when running on AWS infrastructure
```

### Cache Configuration
```python
from tfm_s3 import configure_s3_cache, get_s3_cache_stats

# Configure cache with custom settings
configure_s3_cache(ttl=120, max_entries=2000)

# Check cache statistics
stats = get_s3_cache_stats()
print(f"Cache entries: {stats['total_entries']}")
```

## Usage Examples

### Basic Path Operations
```python
from tfm_path import Path

# Create S3 paths
bucket = Path('s3://my-bucket/')
file_path = Path('s3://my-bucket/documents/report.pdf')

# Path properties
print(file_path.name)        # 'report.pdf'
print(file_path.suffix)     # '.pdf'
print(file_path.parent)     # 's3://my-bucket/documents/'

# Path manipulation
new_file = file_path.with_name('summary.pdf')
csv_file = file_path.with_suffix('.csv')
```

### File Operations with Automatic Caching
```python
# All operations automatically use caching
s3_path = Path('s3://my-bucket/my-file.txt')

# First call hits API and caches result
exists1 = s3_path.exists()  # API call made

# Second call uses cached result
exists2 = s3_path.exists()  # No API call

# Write operation invalidates cache
s3_path.write_text("new content")  # Cache invalidated

# Next call hits API again
exists3 = s3_path.exists()  # API call made
```

### Directory Operations
```python
# List bucket contents (cached)
bucket = Path('s3://my-bucket/')
for item in bucket.iterdir():
    print(f"Found: {item}")

# Create directory (marker)
new_dir = Path('s3://my-bucket/new-folder/')
new_dir.mkdir()

# Search with patterns (uses cached listings)
for pdf_file in bucket.glob('*.pdf'):
    print(f"PDF: {pdf_file}")
```

### Cache Management
```python
from tfm_s3 import clear_s3_cache, get_s3_cache

# Clear all cache entries
clear_s3_cache()

# Get cache instance for advanced operations
cache = get_s3_cache()
cache.invalidate_bucket('my-bucket')

# Manual invalidation
cache.invalidate_key('bucket', 'path/to/file.txt')
cache.invalidate_prefix('bucket', 'path/to/')
```

## TFM Integration

### Navigation
- Navigate to S3 buckets in TFM: `s3://bucket-name/`
- Browse S3 objects like local directories
- Use standard TFM navigation keys
- Cached directory listings for fast browsing

### File Operations
- Copy files between local and S3 storage
- Move and rename S3 objects
- Delete S3 objects with confirmation dialogs
- All operations benefit from intelligent caching

### External Programs
- TFM environment variables work with S3 paths
- `TFM_THIS_DIR` can be an S3 path: `s3://bucket/folder/`
- `TFM_THIS_SELECTED` can include S3 objects
- External scripts can process S3 paths using boto3

### Search and Filter
- Search within S3 buckets using TFM's search functionality
- Filter S3 objects by name patterns
- Content search (if objects are text files)
- Cached search results for improved performance

## Performance Optimization

### Caching Benefits
Based on typical S3 operations:

| Operation | Without Cache | With Cache | Improvement |
|-----------|---------------|------------|-------------|
| exists() | 150-300ms | 1-5ms | 95-98% |
| stat() | 150-300ms | 1-5ms | 95-98% |
| iterdir() | 200-500ms | 10-50ms | 80-95% |
| read_text() | 200-400ms | 50-100ms | 50-75% |

### Memory Usage
- Typical cache entry: 1-10KB
- Default max entries: 1000
- Estimated memory usage: 1-10MB
- LRU eviction prevents unbounded growth

### Configuration Options
```python
# Global configuration
configure_s3_cache(
    ttl=60,          # Default TTL in seconds
    max_entries=1000 # Maximum cache entries
)

# Per-operation TTL
s3_path._cached_api_call(
    'head_object',
    ttl=300,  # 5 minutes
    Bucket='my-bucket',
    Key='my-key'
)
```

## Implementation Details

### S3 Operation Mapping
- **Directories** → Key prefixes with '/' delimiter
- **Files** → S3 objects with keys
- **Paths** → S3 URIs (s3://bucket/key)
- **Listing** → list_objects_v2 with prefix/delimiter (cached)
- **Stat** → head_object for metadata (cached)

### Thread Safety
The entire system is thread-safe:
- Uses `threading.RLock` for all cache operations
- Supports concurrent read/write operations
- Safe for use in multi-threaded TFM environment

### Error Handling
- **Credential Errors**: Clear messages when AWS credentials missing
- **Permission Errors**: Proper handling of access denied scenarios
- **Network Errors**: Graceful handling of connection issues
- **S3 Errors**: Specific handling for NoSuchBucket, NoSuchKey, etc.
- **Cache Failures**: Cache failures don't affect S3 operations

### Cache Invalidation Strategies
```python
def _invalidate_cache_for_write(self, key: Optional[str] = None):
    target_key = key or self._key
    
    # Invalidate the specific key
    self._cache.invalidate_key(self._bucket, target_key)
    
    # Invalidate parent directory listings
    if '/' in target_key:
        parent_key = '/'.join(target_key.split('/')[:-1]) + '/'
        self._cache.invalidate_key(self._bucket, parent_key)
    
    # Invalidate bucket root listing if top-level key
    if '/' not in target_key.strip('/'):
        self._cache.invalidate_key(self._bucket, '')
```

## Monitoring and Statistics

### Available Statistics
```python
stats = get_s3_cache_stats()
# Returns:
{
    'total_entries': int,      # Current cache entries
    'expired_entries': int,    # Expired but not cleaned entries
    'max_entries': int,        # Maximum allowed entries
    'default_ttl': int         # Default TTL in seconds
}
```

### Best Practices
- **Short-lived data**: 30-60 seconds TTL
- **Stable data**: 300-600 seconds TTL
- **Read-heavy workloads**: Longer TTL
- **Write-heavy workloads**: Shorter TTL
- Monitor `expired_entries` for cache efficiency

## Limitations

### S3 Constraints
- **No Symbolic Links**: S3 doesn't support symlinks
- **No Hard Links**: S3 doesn't support hard links
- **No File Permissions**: chmod() is a no-op
- **No True Directories**: Directory operations are simulated

### Performance Considerations
- **Network Latency**: S3 operations slower than local file operations
- **API Rate Limits**: AWS S3 has request rate limits
- **Large Listings**: Very large buckets may be slow to browse initially

### AWS Costs
- **API Requests**: Each operation incurs AWS charges (reduced by caching)
- **Data Transfer**: Downloading/uploading data has costs
- **Storage**: S3 storage charges apply

## Security Considerations

### Credentials
- **Never Hardcode**: Don't embed AWS credentials in code
- **Use IAM Roles**: Preferred method for AWS infrastructure
- **Least Privilege**: Grant minimal required permissions

### Permissions
- **Bucket Access**: Requires appropriate S3 bucket permissions
- **Object Access**: Requires read/write permissions for objects
- **Cross-Account**: May require additional configuration

## Troubleshooting

### Common Issues

#### Credentials Not Found
```
Error: AWS credentials not found
Solution: Configure AWS credentials using 'aws configure' or environment variables
```

#### Permission Denied
```
Error: Access Denied
Solution: Check IAM permissions for the S3 bucket and objects
```

#### High Memory Usage
```python
# Reduce cache size
configure_s3_cache(max_entries=500)

# Or clear cache periodically
clear_s3_cache()
```

#### Stale Data
```python
# Reduce TTL for frequently changing data
configure_s3_cache(ttl=30)

# Or manually invalidate
cache = get_s3_cache()
cache.invalidate_bucket('frequently-changing-bucket')
```

### Debug Information
Enable debug logging to monitor cache behavior:
```python
import logging
logging.basicConfig(level=logging.DEBUG)

# Cache operations will be logged
s3_path.exists()  # Logs cache hit/miss information
```

## Testing

### Test Coverage
- **Unit Tests**: All S3PathImpl methods and S3Cache functionality
- **Integration Tests**: Real AWS operations (when credentials available)
- **Compatibility Tests**: Local path functionality preserved
- **Cache Tests**: Cache invalidation, expiration, and LRU eviction

### Running Tests
```bash
# Unit tests (no AWS credentials required)
python test/test_s3_path.py
python test/test_s3_caching.py

# Integration tests (requires AWS credentials)
python test/test_s3_integration.py
```

## Future Enhancements

### Planned Features
1. **Persistent cache** - Disk-based cache for session persistence
2. **Multi-part upload** - Support for large file uploads
3. **Presigned URLs** - Generate shareable links
4. **S3 Select** - Query data directly in S3
5. **Versioning** - Support for S3 object versioning
6. **Progress indicators** - Show upload/download progress

### Additional Storage Backends
The modular architecture enables easy addition of new storage backends:
- **tfm_scp.py**: SCP/SFTP support
- **tfm_ftp.py**: FTP support
- **tfm_azure.py**: Azure Blob Storage
- **tfm_gcs.py**: Google Cloud Storage

## S3-Specific Features

### Directory Rename Restriction

TFM prevents users from renaming directories on S3 storage to avoid confusion and expensive operations. Unlike local file systems where directory renaming is a simple metadata operation, S3 directory renaming would require copying all objects within the directory and then deleting the originals, which can be:

- **Expensive**: Each object copy and delete operation incurs S3 API costs
- **Slow**: Large directories with many objects could take a very long time
- **Risky**: Potential for partial failures leaving the directory in an inconsistent state

#### Implementation
The restriction is implemented at two levels:

**Dialog Prevention (Primary UX)**:
```python
def enter_rename_mode(self):
    # Check if this storage implementation supports directory renaming
    try:
        if selected_file.is_dir() and not selected_file.supports_directory_rename():
            print("Directory renaming is not supported on this storage type due to performance and cost considerations")
            return
    except Exception as e:
        print(f"Warning: Could not check directory rename capability: {e}")
```

**Backend Protection (Fallback)**:
```python
def rename(self, target) -> 'Path':
    """Rename this file or directory to the given target"""
    # Check if this is a directory - S3 directory renaming is not supported
    if self.is_dir():
        raise OSError("Directory renaming is not supported on S3 due to performance and cost considerations")
```

#### Behavior
- **Files**: Renaming S3 files continues to work as before (copy + delete)
- **Directories**: Attempting to rename any S3 directory shows immediate error message
- **Virtual Directories**: Both explicit directories (ending with `/`) and virtual directories are blocked

### File Editing Capability Indicator

TFM provides a capability indicator for S3 file editing operations through the `supports_file_editing()` method. This allows applications to check whether a storage implementation supports file editing characteristics, without blocking the operations.

#### Implementation
```python
# S3PathImpl returns False to indicate different editing characteristics
def supports_file_editing(self) -> bool:
    return False

# LocalPathImpl returns True for full editing support
def supports_file_editing(self) -> bool:
    return True
```

#### Behavior
- **All S3 file operations work normally**: `open()`, `write_text()`, `write_bytes()`, etc.
- **Capability indicator**: Applications can check `path.supports_file_editing()` to understand storage characteristics
- **FileManager integration**: TFM shows message "Editing S3 files is not supported for now" when launching external editors
- **Non-blocking**: The capability is purely informational - operations work regardless

#### Usage Example
```python
path = Path('s3://bucket/file.txt')
if path.supports_file_editing():
    # Local file system - full editing support expected
    path.write_text("new content")
else:
    # S3 or other storage - editing works but may have different characteristics
    print("Note: This storage type has different editing characteristics")
    path.write_text("new content")  # Still works
```

### Virtual Directory Stats Enhancement

TFM provides meaningful size and timestamp information for S3 virtual directories instead of showing "---" for both values.

#### Problem Solved
Virtual directories in S3 (directories that exist only because there are S3 objects with that prefix) previously showed:
- Size: "---" 
- Date: "---"

#### Solution
- **Size**: Always "0B" for virtual directories (logical since they don't consume storage)
- **Timestamp**: Latest modification time among all child objects
- **Fallback**: Current time if no children exist or timestamps unavailable

#### Implementation
```python
def _get_virtual_directory_stats(self) -> Tuple[int, float]:
    """Get generated stats for virtual directories."""
    # Lists objects under the directory prefix
    # Finds the latest LastModified timestamp among all children
    # Handles pagination for large directories (>1000 objects)
    # Uses caching to optimize performance
    # Returns (size=0, latest_timestamp)
```

#### Performance Features
- **Caching**: Directory stats cached for 30 seconds
- **Efficient Pagination**: Uses S3 paginator for large directories
- **Limited Scope**: Only processes objects needed for timestamp calculation
- **Fallback Strategy**: Quick defaults if API calls fail

#### User Experience Improvement
**Before**:
```
s3://bucket/reports/2024/     ---      ---
s3://bucket/data/processed/   ---      ---
```

**After**:
```
s3://bucket/reports/2024/     0B       2024-06-30 17:45:30
s3://bucket/data/processed/   0B       2024-09-15 09:22:15
```

## S3 Fixes and Performance Optimizations

### Cache System Fixes

#### S3 Cache Key Consistency Fix
**Problem**: `get_file_info()` calls were not hitting the cache and causing 404 errors from HeadObject API calls during directory rendering.

**Root Cause**: Cache keys used during `iterdir()` didn't match the cache keys used during `stat()` calls.

**Solution**: 
- Added `cache_key_override` parameter to `_cached_api_call()` method
- Ensured consistent cache key usage between directory listing and stat operations
- Proactive caching of stat information during directory listing

**Benefits**:
- **API Call Reduction**: 100% for cached files
- **Error Elimination**: 0 404 errors for valid files
- **Performance**: Significantly faster directory operations

#### S3 Caching Performance Optimization
**Problem**: N+1 API call problem causing slow directory rendering (1 `list_objects_v2` call + N `head_object` calls for N files).

**Solution**: 
- Cache file stat information during directory listing
- Use cached metadata from `list_objects_v2` response for subsequent `stat()` calls
- Optimize virtual directory stats to use cached data

**Performance Improvements**:

| Scenario | Before | After | Improvement |
|----------|--------|-------|-------------|
| Directory with 20 files | 21 calls | 1 call | 95% reduction |
| Directory with 100 files | 101 calls | 1 call | 99% reduction |
| Repeated directory access | N+1 calls | 0 calls | 100% reduction |

#### S3 Cache TTL Configuration
**Enhancement**: Made S3 cache TTL configurable through TFM configuration system.

**Configuration**:
```python
class Config:
    S3_CACHE_TTL = 120  # Cache for 2 minutes (default: 60)
```

**Recommended TTL Values**:
- **30 seconds**: Very fresh data, more API calls
- **60 seconds**: Default, good balance
- **120 seconds**: Less API calls, slightly stale data
- **300 seconds**: Good for stable directories
- **600 seconds**: Minimal API calls, longer staleness

#### S3 Cache Invalidation Feature
**Enhancement**: Automatic cache invalidation after file and archive operations.

**Invalidation Strategies**:
- **Copy Operations**: Invalidate destination directory cache
- **Move Operations**: Invalidate both source and destination caches
- **Delete Operations**: Invalidate parent directory cache
- **Archive Operations**: Invalidate archive and source/destination paths
- **Create Operations**: Invalidate created path and parent directory

### Navigation and Path Fixes

#### S3 Backspace Navigation Fix
**Problem**: Backspace key would not work correctly when browsing S3 buckets, particularly for paths ending with trailing slashes.

**Root Cause**: The `parent` property didn't properly handle S3 keys ending with trailing slashes.

**Solution**:
```python
@property
def parent(self) -> 'Path':
    # Strip trailing slash to handle directory keys properly
    key_without_trailing_slash = self._key.rstrip('/')
    
    if '/' not in key_without_trailing_slash:
        return Path(f's3://{self._bucket}/')
    
    parent_key = '/'.join(key_without_trailing_slash.split('/')[:-1])
    if parent_key:
        return Path(f's3://{self._bucket}/{parent_key}/')
    else:
        return Path(f's3://{self._bucket}/')
```

#### S3 Empty Names Fix
**Problem**: Directories were appearing with empty names and showing as "0B" in size.

**Root Cause**: When S3 directory keys end with a forward slash (e.g., `test1/`), the `name` property would return an empty string.

**Solution**:
```python
@property
def name(self) -> str:
    # Strip trailing slash before splitting to handle directory keys properly
    key_without_slash = self._key.rstrip('/')
    return key_without_slash.split('/')[-1] if '/' in key_without_slash else key_without_slash
```

### File Operation Fixes

#### S3 Copy Fix
**Problem**: Copying files from local filesystem to S3 resulted in "Permission denied" errors.

**Root Cause**: TFM was using `shutil.copy2()` for all copy operations, which only works with local filesystem paths.

**Solution**: 
- Added new `copy_to()` method to Path class
- Implemented cross-storage copy logic
- Automatic storage type detection

**Cross-Storage Copy Support**:
- **Local to Local**: Uses `shutil.copy2()` for optimal performance
- **Local to S3**: Reads file content and uploads using `write_bytes()`
- **S3 to Local**: Downloads using `read_bytes()` and writes locally
- **S3 to S3**: Downloads from source and uploads to destination

#### S3 Move Fix
**Problem**: Moving files between S3 directories resulted in "No such file or directory" errors.

**Root Cause**: Move operations were using `shutil.move()` which doesn't understand S3 URIs.

**Solution**: 
- Replace `shutil.move()` calls with `Path.rename()` calls
- Updated directory removal logic to use Path methods
- Enhanced error handling for S3-specific issues

#### S3 Directory Deletion Fix
**Problem**: Attempting to delete S3 directories resulted in "No files to delete" error.

**Root Cause**: 
1. `exists()` method only checked for actual S3 objects, not virtual directories
2. Lack of recursive deletion support for S3 paths

**Solution**:
- Enhanced `exists()` method to check for virtual directories
- Added `rmtree()` method for recursive S3 directory deletion
- Added `_delete_objects_batch()` for efficient batch deletion
- Enhanced TFM directory deletion logic for S3 paths

### Virtual Directory Optimizations

#### S3 Virtual Directory Optimization
**Problem**: Virtual directories (directories without actual S3 objects) caused HeadObject failures and unnecessary API calls.

**Solution**: Store metadata as S3PathImpl instance properties to eliminate API calls.

**Implementation**:
```python
def __init__(self, s3_uri: str, metadata: Optional[Dict[str, Any]] = None):
    self._metadata = metadata or {}
    self._is_dir_cached = self._metadata.get('is_dir')
    self._is_file_cached = self._metadata.get('is_file')
    self._size_cached = self._metadata.get('size')
    self._mtime_cached = self._metadata.get('last_modified')
```

**Performance Improvements**:

| Operation | Before | After | Improvement |
|-----------|--------|-------|-------------|
| `is_dir()` on virtual directory | 1 API call | 0 API calls | 100% reduction |
| `is_file()` on cached file | 1 API call | 0 API calls | 100% reduction |
| `stat()` on cached object | 1 API call | 0 API calls | 100% reduction |
| Directory with 20 items | 20+ API calls | 0 API calls | 100% reduction |

### Overall Performance Impact

#### Performance Improvements Summary

| Metric | Improvement | Impact |
|--------|-------------|---------|
| API Calls | 90-99% reduction | Faster operations, lower costs |
| Directory Rendering | 50-90% faster | Better user experience |
| Cache Hit Rate | 95%+ for repeated operations | Near-instant responses |
| Error Rate | 100% reduction for virtual directories | More reliable operations |

#### Memory Usage
- **Cache Overhead**: 1-10MB for typical usage
- **Metadata Storage**: Minimal overhead (few KB per object)
- **LRU Eviction**: Prevents unbounded growth

#### Cost Savings
- **AWS API Costs**: Reduced by 90-99% for directory operations
- **Request Charges**: Significant reduction in billable API requests
- **Data Transfer**: Minimal impact (metadata is small)

## Related Documentation
- [TFM Path Architecture](TFM_PATH_ARCHITECTURE.md)
- [External Programs Policy](../external-programs-policy.md)
- [AWS S3 Documentation](https://docs.aws.amazon.com/s3/)
- [boto3 Documentation](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)